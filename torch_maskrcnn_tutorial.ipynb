{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20d285de-3bab-4c3d-b0b6-80b559dac2e0",
   "metadata": {},
   "source": [
    "# Segmentation fine-tuning tutorial\n",
    "Tutorial from: \n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "For this tutorial, we will be finetuning a pre-trained Mask R-CNN model on the Penn-Fudan Database for Pedestrian Detection and Segmentation. It contains 170 images with 345 instances of pedestrians, and we will use it to illustrate how to use the new features in torchvision in order to train an object detection and instance segmentation model on a custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c790aae8-809a-4130-9a35-398686be151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f04b3f-e2f8-4534-b433-c9d056cd7211",
   "metadata": {},
   "source": [
    "# Defining the Dataset\n",
    "The reference scripts for training object detection, instance segmentation and person keypoint detection allows for easily supporting adding new custom datasets. The dataset should inherit from the standard `torch.utils.data.Dataset` class, and implement `__len__` and `__getitem__`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a1e493-002f-4dc9-8dc1-b034201fad38",
   "metadata": {},
   "source": [
    "## Writing a custom dataset for PennFudan\n",
    "Let’s write a dataset for the PennFudan dataset. After downloading and extracting the zip file, the data will include the following (in addition to some others).\n",
    "\n",
    "    PennFudanPed/\n",
    "        Annotation/  # includes bounding boxes\n",
    "            FudanPed00001.txt\n",
    "            FudanPed00002.txt\n",
    "            FudanPed00003.txt\n",
    "            FudanPed00004.txt\n",
    "            ...\n",
    "        \n",
    "        PedMasks/  # includes segmentation masks\n",
    "            FudanPed00001_mask.png\n",
    "            FudanPed00002_mask.png\n",
    "            FudanPed00003_mask.png\n",
    "            FudanPed00004_mask.png\n",
    "            ...\n",
    "        PNGImages/  # includes images \n",
    "            FudanPed00001.png\n",
    "            FudanPed00002.png\n",
    "            FudanPed00003.png\n",
    "            FudanPed00004.png\n",
    "            ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4118d-dcf4-4fa1-a530-ed1f3b874cb0",
   "metadata": {},
   "source": [
    "Path to penn dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e45f3-8455-4646-9c0d-8b06cdefe8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'C:/Users/Eric/development/data/PennFudanPed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a606a6a6-5060-441b-961e-4a0595c73be4",
   "metadata": {},
   "source": [
    "Dataset class just gets N masks (N binary masks, encoded as uint8, bounding boxes and their areas, labels, and images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f1e4d1-8dcf-4c8e-a032-b2a7e2dc26a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.images = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        image_path = os.path.join(self.root, \"PNGImages\", self.images[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        image = read_image(image_path)\n",
    "        mask = read_image(mask_path)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = torch.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        # split the color-encoded mask into a set of binary masks\n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        boxes = masks_to_boxes(masks)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        image_id = idx\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Wrap sample and targets into torchvision tv_tensors:\n",
    "        image = tv_tensors.Image(image)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(image))\n",
    "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5029864-a488-4eb2-9083-2e3a0d8779b5",
   "metadata": {},
   "source": [
    "# Defining model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c47d0eb-4359-4f48-85e7-f135420297a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We will start from a pre-trained model trained on the coco dataset, and fine-tune the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc70078-93de-4581-8aa8-270e4654e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    \n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbb9067-ee10-4bff-9a89-1fee28593c98",
   "metadata": {},
   "source": [
    "That’s it, this will make model be ready to be trained and evaluated on your custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f64877-3927-45ba-acb3-c042c4e8620e",
   "metadata": {},
   "source": [
    "## Pulling in bits for training\n",
    "There are some helper functions to simplify training and evaluating detection models. Here, we will use `engine.py` and `utils.py` from our object detection reference repo.\n",
    "\n",
    "Just download everything under https://github.com/pytorch/vision/tree/main/references/detection to your folder and use them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45435f83-e0d6-462a-ae21-c3aba02c0a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25426c1-a66e-41f0-81c4-03133992df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77289ba0-8bc2-4e5b-ab98-09beb7f30815",
   "metadata": {},
   "source": [
    "Following is necessary when working in windows, otherwise can set to 4. :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e749260a-b08d-4ef3-b62d-fe8ff4a36f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_torch_workers = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e46da7-858b-48e6-83b7-b24af7afbc5d",
   "metadata": {},
   "source": [
    "Wrapping dataset into dataloader.\n",
    "Note we haven't discussed data loader. It is discussed, in context of dataset, here:\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce0078b-5da3-4003-804a-bc2097918652",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "dataset = PennFudanDataset(data_path, get_transform(train=True))\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, num_workers=num_torch_workers, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4778cceb-d062-4e01-a7b5-b308c7fde57f",
   "metadata": {},
   "source": [
    "Test getitem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1a782a-6dc7-4a5b-8d3a-c3143447598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im0, targ0 = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8f958-a8e8-4f23-8947-3e7d2e2ce013",
   "metadata": {},
   "source": [
    "For Training we use the iterable dataloader (for more see https://stackoverflow.com/a/77654159/1886357).\n",
    "\n",
    "Aside: Standard kinds of syntax (you wouldn't typically use `next(iter(loader))`, but would have it in a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237dc101-1ecd-4ad4-8479-68c3da9a4e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(data_loader))\n",
    "type(images), type(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e449e89-4d2f-49f7-9e62-930dafc14f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = list(image for image in images)\n",
    "# list of target dictionaries\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "len(images), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185aeb7-10a1-441a-ab49-d2f5e80287a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure __getitem__ worked\n",
    "targets[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d061420-5afe-4b56-a4f2-aec061bf9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].shape, images[0].permute(1, 2, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fceca5f-88e6-4009-85bc-2fe41efa2b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0].permute(1,2,0));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbd3929-2693-4cc9-b58b-2304972dee1c",
   "metadata": {},
   "source": [
    "## Getting more serious: defining functions for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702bb278-7dfc-416d-8b12-0d2a5040f286",
   "metadata": {},
   "source": [
    "Let’s now write the main function which performs the training and the validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848a3c1d-3a45-47e4-a9e5-2f211a5cd035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c2708-7e64-41e7-8f14-df413523d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec5503b-c6f7-46d4-87f4-9b90150558c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = PennFudanDataset(data_path, get_transform(train=True))\n",
    "dataset_test = PennFudanDataset(data_path, get_transform(train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3838ddd6-f0c4-4cdc-bcd2-dc0f375c9653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset in train and test set (50 test sessions)\n",
    "# this could be done much better\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471addb8-4133-4a55-895b-f7c1523a0271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,  #2\n",
    "    shuffle=True,\n",
    "    num_workers=num_torch_workers,\n",
    "    collate_fn=utils.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5162b4c2-fde5-47bb-9bc1-98eeba0d6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=num_torch_workers,\n",
    "    collate_fn=utils.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f940e-5e47-4903-a4af-1a01963efeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model using our helper function\n",
    "model = get_model_instance_segmentation(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aef25fd-0b6c-4323-aab3-c6bf35819102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model to the right device\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2005be37-72b7-45bd-8212-e36b1d53b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30332ba4-6e64-422f-b7a4-646075a737e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc07f06e-aed7-44d3-8c9e-1976cfe3db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a4a40-0338-4e46-9af1-4d6518697789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train it for 5 epochs just making sure things work\n",
    "num_epochs = 1 #5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21a61b-2116-4535-b2c2-d0e219d2374e",
   "metadata": {},
   "source": [
    "On the evaluation metrics:    \n",
    "https://debuggercafe.com/evaluation-metrics-for-object-detection/\n",
    "\n",
    "## Let's run training\n",
    "\n",
    "**Note added**   \n",
    "- The following doesn't seem built for practical ML eg for early stopping** -- this is what I want to change!\n",
    "- Also is it weird it doesn't normalize/standardize the images like most of these faster-rcnn network tutorials do? Is that an issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790f278-d2e7-483e-a50d-84726e85ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=20)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    torch.cuda.empty_cache()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n**DONE!**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d713c6e4-104b-44be-8d5e-1a09b6a0164a",
   "metadata": {},
   "source": [
    "So after one epoch of training, we obtain a COCO-style mAP > 50, and a mask mAP of 65.  \n",
    "\n",
    "**Note added**    \n",
    "- Arguably the most important part of the fine-tuning tutorial, the above is not very helpful. No plots of learning, using different metrics for training/validation, no discussion of validation or early stopping, or practical matters  -- how am I supposed to use this tutorial with my actual data?\n",
    "- My GPU is filled with stuff right now. How do I clear it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31021553-71bb-445c-8219-e0753384cffc",
   "metadata": {},
   "source": [
    "## Inspect results\n",
    "But what do the predictions look like? Let’s take one image in the dataset and verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c4386-a73a-4080-809e-db519efb2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2218586e-2eb3-4a87-bd6f-ffa1ff1b3aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = read_image(\"./tv_image05.png\")\n",
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776a4446-c5bb-473e-b2ae-7921eb1ae4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_transform = get_transform(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1214a75-6f52-4c38-ab9f-f01ba39c7e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = eval_transform(image)\n",
    "    # convert RGBA -> RGB and move to device\n",
    "    x = x[:3, ...].to(device)\n",
    "    predictions = model([x, ])\n",
    "    pred = predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813824c3-a323-45ad-a048-433692efc1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
    "image = image[:3, ...]\n",
    "pred_labels = [f\"pedestrian: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n",
    "pred_boxes = pred[\"boxes\"].long()\n",
    "output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n",
    "masks = (pred[\"masks\"] > 0.7).squeeze(1)\n",
    "output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551c04a-cc50-49e4-b619-31db95a82b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(output_image.permute(1, 2, 0));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dfd11b-d710-4ea6-939e-b51b76ee0dca",
   "metadata": {},
   "source": [
    "The results look good!\n",
    "\n",
    "**Note added**    \n",
    "- Really? The eyeball test is our benchmark?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b233796-a336-49ff-b60b-009bbdfdc3e7",
   "metadata": {},
   "source": [
    "## Wrapping up \n",
    "In this tutorial, you have learned how to create your own training pipeline for object detection models on a custom dataset. For that, you wrote a torch.utils.data.Dataset class that returns the images and the ground truth boxes and segmentation masks. You also leveraged a Mask R-CNN model pre-trained on COCO train2017 in order to perform transfer learning on this new dataset.\n",
    "\n",
    "For a more complete example, which includes multi-machine / multi-GPU training, check references/detection/train.py, which is present in the torchvision repository:\n",
    "\n",
    "https://github.com/pytorch/vision/blob/main/references/detection/train.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
